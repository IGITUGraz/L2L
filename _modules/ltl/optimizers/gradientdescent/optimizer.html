<!DOCTYPE html>




<html lang="en">
  <head>
    <meta charset="utf-8" />
    
    <title>ltl.optimizers.gradientdescent.optimizer &mdash; LTL 1.0.0-beta documentation</title>
    <meta name="description" content="">
    <meta name="author" content="">

    

<link rel="stylesheet" href="../../../../_static/css/basicstrap-base.css" type="text/css" />
<link rel="stylesheet" id="current-theme" href="../../../../_static/css/bootstrap3/bootstrap.min.css" type="text/css" />
<link rel="stylesheet" id="current-adjust-theme" type="text/css" />

<link rel="stylesheet" href="../../../../_static/css/font-awesome.min.css">

<style type="text/css">
  body {
    padding-top: 60px;
    padding-bottom: 40px;
  }
</style>

<link rel="stylesheet" href="../../../../_static/css/basicstrap.css" type="text/css" />
<link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
    
<script type="text/javascript">
  var DOCUMENTATION_OPTIONS = {
            URL_ROOT:    '../../../../',
            VERSION:     '1.0.0-beta',
            COLLAPSE_INDEX: false,
            FILE_SUFFIX: '.html',
            HAS_SOURCE:  true
  };
</script>
    <script type="text/javascript" src="../../../../_static/js/jquery.min.js"></script>
    <script type="text/javascript" src="../../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/javascript" src="../../../../_static/js/bootstrap3.min.js"></script>
<script type="text/javascript" src="../../../../_static/js/jquery.cookie.min.js"></script>
<script type="text/javascript" src="../../../../_static/js/basicstrap.js"></script>
<script type="text/javascript">
</script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
    <link rel="top" title="LTL 1.0.0-beta documentation" href="../../../../index.html" />
    <link rel="up" title="ltl" href="../../../ltl.html" /> 
  </head>
  <body role="document">
    <div id="navbar-top" class="navbar navbar-fixed-top navbar-default" role="navigation" aria-label="top navigation">
      <div class="container-fluid">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="../../../../index.html">LTL 1.0.0-beta documentation</a>
        </div>
        <div class="navbar-collapse collapse">
          <ul class="nav navbar-nav navbar-right">

            
              <li><a href="../../../../py-modindex.html" title="Python Module Index" >modules </a></li>
              <li><a href="../../../../genindex.html" title="General Index" accesskey="I">index </a></li>
              <li><a href="../../../index.html" >Module code</a></li>
              <li><a href="../../../ltl.html" accesskey="U">ltl</a></li>
            

            <li class="visible-xs">
                <form class="search form-search form-inline navbar-form navbar-right sp-searchbox" action="../../../../search.html" method="get">
                  <div class="input-append input-group">
                    <input type="text" class="search-query form-control" name="q" placeholder="Search...">
                    <span class="input-group-btn">
                    <input type="submit" class="btn" value="Go" />
                    </span>
                  </div>
                  <input type="hidden" name="check_keywords" value="yes" />
                  <input type="hidden" name="area" value="default" />
                </form>
            </li>

            

          </ul>

        </div>
      </div>
    </div>
    

    <!-- container -->
    <div class="container-fluid">

      <!-- row -->
      <div class="row">
         
<div class="col-md-3 hidden-xs" id="sidebar-wrapper">
  <div class="sidebar hidden-xs" role="navigation" aria-label="main navigation">
<h3><a href="../../../../index.html">Table of Contents</a></h3>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../intro.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ltl.html">API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ltl-bin.html">LTL Experiments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../indices.html">Indices and tables</a></li>
</ul>

<div id="searchbox" role="search">
  <h3>Quick search</h3>
  <form class="search form-inline" action="../../../../search.html" method="get">
      <div class="input-append input-group">
        <input type="text" class="search-query form-control" name="q" placeholder="Search...">
        <span class="input-group-btn">
        <input type="submit" class="btn" value="Go" />
        </span>
      </div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
  </div>
</div> 
        

        <div class="col-md-9" id="content-wrapper">
          <div class="document" role="main">
            <div class="documentwrapper">
              <div class="bodywrapper">
                <div class="body">
                  
  <h1>Source code for ltl.optimizers.gradientdescent.optimizer</h1><div class="highlight"><pre>
<span></span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="k">import</span> <span class="n">namedtuple</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">ltl</span> <span class="k">import</span> <span class="n">dict_to_list</span>
<span class="kn">from</span> <span class="nn">ltl</span> <span class="k">import</span> <span class="n">list_to_dict</span>
<span class="kn">from</span> <span class="nn">ltl.optimizers.optimizer</span> <span class="k">import</span> <span class="n">Optimizer</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s2">&quot;optimizers.gradientdescent&quot;</span><span class="p">)</span>

<span class="n">ClassicGDParameters</span> <span class="o">=</span> <span class="n">namedtuple</span><span class="p">(</span>
    <span class="s1">&#39;ClassicGDParameters&#39;</span><span class="p">,</span>
    <span class="p">[</span><span class="s1">&#39;learning_rate&#39;</span><span class="p">,</span> <span class="s1">&#39;exploration_step_size&#39;</span><span class="p">,</span> <span class="s1">&#39;n_random_steps&#39;</span><span class="p">,</span> <span class="s1">&#39;n_iteration&#39;</span><span class="p">,</span> <span class="s1">&#39;stop_criterion&#39;</span><span class="p">,</span> <span class="s1">&#39;seed&#39;</span><span class="p">])</span>
<span class="n">ClassicGDParameters</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">:param learning_rate: The rate of learning per step of gradient descent</span>
<span class="s2">:param exploration_step_size: The standard deviation of random steps used for finite difference gradient</span>
<span class="s2">:param n_random_steps: The amount of random steps used to estimate gradient</span>
<span class="s2">:param n_iteration: number of iteration to perform</span>
<span class="s2">:param stop_criterion: Stop if change in fitness is below this value</span>
<span class="s2">&quot;&quot;&quot;</span>

<span class="n">StochasticGDParameters</span> <span class="o">=</span> <span class="n">namedtuple</span><span class="p">(</span>
    <span class="s1">&#39;StochasticGDParameters&#39;</span><span class="p">,</span>
    <span class="p">[</span><span class="s1">&#39;learning_rate&#39;</span><span class="p">,</span> <span class="s1">&#39;stochastic_deviation&#39;</span><span class="p">,</span> <span class="s1">&#39;stochastic_decay&#39;</span><span class="p">,</span> <span class="s1">&#39;exploration_step_size&#39;</span><span class="p">,</span> <span class="s1">&#39;n_random_steps&#39;</span><span class="p">,</span> <span class="s1">&#39;n_iteration&#39;</span><span class="p">,</span>
     <span class="s1">&#39;stop_criterion&#39;</span><span class="p">,</span> <span class="s1">&#39;seed&#39;</span><span class="p">])</span>
<span class="n">StochasticGDParameters</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">:param learning_rate: The rate of learning per step of gradient descent</span>
<span class="s2">:param stochastic_deviation: The standard deviation of the random vector used to perturbate the gradient</span>
<span class="s2">:param stochastic_decay: The decay of the influence of the random vector that is added to the gradient </span>
<span class="s2">    (set to 0 to disable stochastic perturbation)</span>
<span class="s2">:param exploration_step_size: The standard deviation of random steps used for finite difference gradient</span>
<span class="s2">:param n_random_steps: The amount of random steps used to estimate gradient</span>
<span class="s2">:param n_iteration: number of iteration to perform</span>
<span class="s2">:param stop_criterion: Stop if change in fitness is below this value</span>
<span class="s2">&quot;&quot;&quot;</span>

<span class="n">AdamParameters</span> <span class="o">=</span> <span class="n">namedtuple</span><span class="p">(</span>
    <span class="s1">&#39;AdamParameters&#39;</span><span class="p">,</span>
    <span class="p">[</span><span class="s1">&#39;learning_rate&#39;</span><span class="p">,</span> <span class="s1">&#39;exploration_step_size&#39;</span><span class="p">,</span> <span class="s1">&#39;n_random_steps&#39;</span><span class="p">,</span> <span class="s1">&#39;first_order_decay&#39;</span><span class="p">,</span> <span class="s1">&#39;second_order_decay&#39;</span><span class="p">,</span> <span class="s1">&#39;n_iteration&#39;</span><span class="p">,</span>
     <span class="s1">&#39;stop_criterion&#39;</span><span class="p">,</span> <span class="s1">&#39;seed&#39;</span><span class="p">])</span>
<span class="n">AdamParameters</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">:param learning_rate: The rate of learning per step of gradient descent</span>
<span class="s2">:param exploration_step_size: The standard deviation of random steps used for finite difference gradient</span>
<span class="s2">:param n_random_steps: The amount of random steps used to estimate gradient</span>
<span class="s2">:param first_order_decay: Specifies the amount of decay of the historic first order momentum per gradient descent step</span>
<span class="s2">:param second_order_decay: Specifies the amount of decay of the historic second order momentum per gradient descent step</span>
<span class="s2">:param n_iteration: number of iteration to perform</span>
<span class="s2">:param stop_criterion: Stop if change in fitness is below this value</span>

<span class="s2">&quot;&quot;&quot;</span>

<span class="n">RMSPropParameters</span> <span class="o">=</span> <span class="n">namedtuple</span><span class="p">(</span>
    <span class="s1">&#39;RMSPropParameters&#39;</span><span class="p">,</span>
    <span class="p">[</span><span class="s1">&#39;learning_rate&#39;</span><span class="p">,</span> <span class="s1">&#39;exploration_step_size&#39;</span><span class="p">,</span> <span class="s1">&#39;n_random_steps&#39;</span><span class="p">,</span> <span class="s1">&#39;momentum_decay&#39;</span><span class="p">,</span> <span class="s1">&#39;n_iteration&#39;</span><span class="p">,</span> <span class="s1">&#39;stop_criterion&#39;</span><span class="p">,</span> <span class="s1">&#39;seed&#39;</span><span class="p">])</span>
<span class="n">RMSPropParameters</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">:param learning_rate: The rate of learning per step of gradient descent</span>
<span class="s2">:param exploration_step_size: The standard deviation of random steps used for finite difference gradient</span>
<span class="s2">:param n_random_steps: The amount of random steps used to estimate gradient</span>
<span class="s2">:param momentum_decay: Specifies the decay of the historic momentum at each gradient descent step</span>
<span class="s2">:param n_iteration: number of iteration to perform</span>
<span class="s2">:param stop_criterion: Stop if change in fitness is below this value</span>
<span class="s2">:param seed: The random seed used for random number generation in the optimizer</span>
<span class="s2">&quot;&quot;&quot;</span>


<div class="viewcode-block" id="GradientDescentOptimizer"><a class="viewcode-back" href="../../../../ltl.optimizers.gradientdescent.html#ltl.optimizers.gradientdescent.optimizer.GradientDescentOptimizer">[docs]</a><span class="k">class</span> <span class="nc">GradientDescentOptimizer</span><span class="p">(</span><span class="n">Optimizer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Class for a generic gradient descent solver.</span>
<span class="sd">    In the pseudo code the algorithm does:</span>

<span class="sd">    For n iterations do:</span>
<span class="sd">        - Explore the fitness of individuals in the close vicinity of the current one</span>
<span class="sd">        - Calculate the gradient based on these fitnesses.</span>
<span class="sd">        - Create the new &#39;current individual&#39; by taking a step in the parameters space along the direction</span>
<span class="sd">            of the largest ascent of the plane</span>

<span class="sd">    NOTE: This expects all parameters of the system to be of floating point</span>

<span class="sd">    :param  ~pypet.trajectory.Trajectory traj:</span>
<span class="sd">      Use this pypet trajectory to store the parameters of the specific runs. The parameters should be</span>
<span class="sd">      initialized based on the values in `parameters`</span>
<span class="sd">    </span>
<span class="sd">    :param optimizee_create_individual:</span>
<span class="sd">      Function that creates a new individual</span>
<span class="sd">    </span>
<span class="sd">    :param optimizee_fitness_weights: </span>
<span class="sd">      Fitness weights. The fitness returned by the Optimizee is multiplied by these values (one for each</span>
<span class="sd">      element of the fitness vector)</span>
<span class="sd">    </span>
<span class="sd">    :param parameters: </span>
<span class="sd">      Instance of :func:`~collections.namedtuple` :class:`.ClassicGDParameters`,</span>
<span class="sd">      :func:`~collections.namedtuple` :class:`.StochasticGDParameters`,</span>
<span class="sd">      :func:`~collections.namedtuple` :class:`.RMSPropParameters` or</span>
<span class="sd">      :func:`~collections.namedtuple` :class:`.AdamParameters` containing the</span>
<span class="sd">      parameters needed by the Optimizer. The type of this parameter is used to select one of the GD variants.</span>
<span class="sd">    </span>
<span class="sd">    :param optimizee_bounding_func:</span>
<span class="sd">      This is a function that takes an individual as argument and returns another individual that is</span>
<span class="sd">      within bounds (The bounds are defined by the function itself). If not provided, the individuals</span>
<span class="sd">      are not bounded.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">traj</span><span class="p">,</span>
                 <span class="n">optimizee_create_individual</span><span class="p">,</span>
                 <span class="n">optimizee_fitness_weights</span><span class="p">,</span>
                 <span class="n">parameters</span><span class="p">,</span>
                 <span class="n">optimizee_bounding_func</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">traj</span><span class="p">,</span>
                         <span class="n">optimizee_create_individual</span><span class="o">=</span><span class="n">optimizee_create_individual</span><span class="p">,</span>
                         <span class="n">optimizee_fitness_weights</span><span class="o">=</span><span class="n">optimizee_fitness_weights</span><span class="p">,</span>
                         <span class="n">parameters</span><span class="o">=</span><span class="n">parameters</span><span class="p">,</span> <span class="n">optimizee_bounding_func</span><span class="o">=</span><span class="n">optimizee_bounding_func</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">recorder_parameters</span> <span class="o">=</span> <span class="n">parameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizee_bounding_func</span> <span class="o">=</span> <span class="n">optimizee_bounding_func</span>
        
        <span class="n">traj</span><span class="o">.</span><span class="n">f_add_parameter</span><span class="p">(</span><span class="s1">&#39;learning_rate&#39;</span><span class="p">,</span> <span class="n">parameters</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">comment</span><span class="o">=</span><span class="s1">&#39;Value of learning rate&#39;</span><span class="p">)</span>
        <span class="n">traj</span><span class="o">.</span><span class="n">f_add_parameter</span><span class="p">(</span><span class="s1">&#39;exploration_step_size&#39;</span><span class="p">,</span> <span class="n">parameters</span><span class="o">.</span><span class="n">exploration_step_size</span><span class="p">,</span> 
                             <span class="n">comment</span><span class="o">=</span><span class="s1">&#39;Standard deviation of the random steps&#39;</span><span class="p">)</span>
        <span class="n">traj</span><span class="o">.</span><span class="n">f_add_parameter</span><span class="p">(</span><span class="s1">&#39;n_random_steps&#39;</span><span class="p">,</span> <span class="n">parameters</span><span class="o">.</span><span class="n">n_random_steps</span><span class="p">,</span> 
                             <span class="n">comment</span><span class="o">=</span><span class="s1">&#39;Amount of random steps taken for calculating the gradient&#39;</span><span class="p">)</span>
        <span class="n">traj</span><span class="o">.</span><span class="n">f_add_parameter</span><span class="p">(</span><span class="s1">&#39;n_iteration&#39;</span><span class="p">,</span> <span class="n">parameters</span><span class="o">.</span><span class="n">n_iteration</span><span class="p">,</span> <span class="n">comment</span><span class="o">=</span><span class="s1">&#39;Number of iteration to perform&#39;</span><span class="p">)</span>
        <span class="n">traj</span><span class="o">.</span><span class="n">f_add_parameter</span><span class="p">(</span><span class="s1">&#39;stop_criterion&#39;</span><span class="p">,</span> <span class="n">parameters</span><span class="o">.</span><span class="n">stop_criterion</span><span class="p">,</span> <span class="n">comment</span><span class="o">=</span><span class="s1">&#39;Stopping criterion parameter&#39;</span><span class="p">)</span>
        <span class="n">traj</span><span class="o">.</span><span class="n">f_add_parameter</span><span class="p">(</span><span class="s1">&#39;seed&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">uint32</span><span class="p">(</span><span class="n">parameters</span><span class="o">.</span><span class="n">seed</span><span class="p">),</span> <span class="n">comment</span><span class="o">=</span><span class="s1">&#39;Optimizer random seed&#39;</span><span class="p">)</span>
        
        <span class="n">_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizee_individual_dict_spec</span> <span class="o">=</span> <span class="n">dict_to_list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizee_create_individual</span><span class="p">(),</span> <span class="n">get_dict_spec</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="n">traj</span><span class="o">.</span><span class="n">par</span><span class="o">.</span><span class="n">seed</span><span class="p">)</span>

        <span class="c1"># Note that this array stores individuals as an np.array of floats as opposed to Individual-Dicts</span>
        <span class="c1"># This is because this array is used within the context of the gradient descent algorithm and</span>
        <span class="c1"># Thus needs to handle the optimizee individuals as vectors</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">current_individual</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dict_to_list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizee_create_individual</span><span class="p">()))</span>

        <span class="c1"># Depending on the algorithm used, initialize the necessary variables</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">updateFunction</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">parameters</span><span class="p">)</span> <span class="ow">is</span> <span class="n">ClassicGDParameters</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">init_classic_gd</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">traj</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">type</span><span class="p">(</span><span class="n">parameters</span><span class="p">)</span> <span class="ow">is</span> <span class="n">StochasticGDParameters</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">init_stochastic_gd</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">traj</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">type</span><span class="p">(</span><span class="n">parameters</span><span class="p">)</span> <span class="ow">is</span> <span class="n">AdamParameters</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">init_adam</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">traj</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">type</span><span class="p">(</span><span class="n">parameters</span><span class="p">)</span> <span class="ow">is</span> <span class="n">RMSPropParameters</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">init_rmsprop</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">traj</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s1">&#39;Class of the provided &quot;parameters&quot; argument is not among the supported types&#39;</span><span class="p">)</span>

        <span class="c1"># Added a generation-wise parameter logging</span>
        <span class="n">traj</span><span class="o">.</span><span class="n">results</span><span class="o">.</span><span class="n">f_add_result_group</span><span class="p">(</span><span class="s1">&#39;generation_params&#39;</span><span class="p">,</span>
                                        <span class="n">comment</span><span class="o">=</span><span class="s1">&#39;This contains the optimizer parameters that are&#39;</span>
                                                <span class="s1">&#39; common across a generation&#39;</span><span class="p">)</span>

        <span class="c1"># Explore the neighbourhood in the parameter space of current individual</span>
        <span class="n">new_individual_list</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">list_to_dict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">current_individual</span> <span class="o">+</span> 
                         <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">parameters</span><span class="o">.</span><span class="n">exploration_step_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">current_individual</span><span class="o">.</span><span class="n">size</span><span class="p">),</span>
                         <span class="bp">self</span><span class="o">.</span><span class="n">optimizee_individual_dict_spec</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">parameters</span><span class="o">.</span><span class="n">n_random_steps</span><span class="p">)</span>
        <span class="p">]</span>

        <span class="c1"># Also add the current individual to determine it&#39;s fitness</span>
        <span class="n">new_individual_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">list_to_dict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">current_individual</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizee_individual_dict_spec</span><span class="p">))</span>
            
        <span class="k">if</span> <span class="n">optimizee_bounding_func</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">new_individual_list</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizee_bounding_func</span><span class="p">(</span><span class="n">ind</span><span class="p">)</span> <span class="k">for</span> <span class="n">ind</span> <span class="ow">in</span> <span class="n">new_individual_list</span><span class="p">]</span>

        <span class="c1"># Storing the fitness of the current individual</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">current_fitness</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">Inf</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">g</span> <span class="o">=</span> <span class="mi">0</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">eval_pop</span> <span class="o">=</span> <span class="n">new_individual_list</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_expand_trajectory</span><span class="p">(</span><span class="n">traj</span><span class="p">)</span>

<div class="viewcode-block" id="GradientDescentOptimizer.get_params"><a class="viewcode-back" href="../../../../ltl.optimizers.gradientdescent.html#ltl.optimizers.gradientdescent.optimizer.GradientDescentOptimizer.get_params">[docs]</a>    <span class="k">def</span> <span class="nf">get_params</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get parameters used for recorder</span>
<span class="sd">        :return: Dictionary containing recorder parameters</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">recorder_parameters</span><span class="o">.</span><span class="n">_asdict</span><span class="p">()</span></div>

<div class="viewcode-block" id="GradientDescentOptimizer.post_process"><a class="viewcode-back" href="../../../../ltl.optimizers.gradientdescent.html#ltl.optimizers.gradientdescent.optimizer.GradientDescentOptimizer.post_process">[docs]</a>    <span class="k">def</span> <span class="nf">post_process</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">traj</span><span class="p">,</span> <span class="n">fitnesses_results</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        See :meth:`~ltl.optimizers.optimizer.Optimizer.post_process`</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">old_eval_pop</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_pop</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eval_pop</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>

        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;  Evaluating </span><span class="si">%i</span><span class="s2"> individuals&quot;</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">fitnesses_results</span><span class="p">))</span>
        
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">fitnesses_results</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">==</span> <span class="n">traj</span><span class="o">.</span><span class="n">n_random_steps</span>

        <span class="c1"># We need to collect the directions of the random steps along with the fitness evaluated there</span>
        <span class="n">fitnesses</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">traj</span><span class="o">.</span><span class="n">n_random_steps</span><span class="p">))</span>
        <span class="n">dx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">traj</span><span class="o">.</span><span class="n">n_random_steps</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">current_individual</span><span class="p">)))</span>
        <span class="n">weighted_fitness_list</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">run_index</span><span class="p">,</span> <span class="n">fitness</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">fitnesses_results</span><span class="p">):</span>
            <span class="c1"># We need to convert the current run index into an ind_idx</span>
            <span class="c1"># (index of individual within one generation</span>
            <span class="n">traj</span><span class="o">.</span><span class="n">v_idx</span> <span class="o">=</span> <span class="n">run_index</span>
            <span class="n">ind_index</span> <span class="o">=</span> <span class="n">traj</span><span class="o">.</span><span class="n">par</span><span class="o">.</span><span class="n">ind_idx</span>
        
            <span class="n">individual</span> <span class="o">=</span> <span class="n">old_eval_pop</span><span class="p">[</span><span class="n">ind_index</span><span class="p">]</span>

            <span class="n">traj</span><span class="o">.</span><span class="n">f_add_result</span><span class="p">(</span><span class="s1">&#39;$set.$.individual&#39;</span><span class="p">,</span> <span class="n">individual</span><span class="p">)</span>
            <span class="n">traj</span><span class="o">.</span><span class="n">f_add_result</span><span class="p">(</span><span class="s1">&#39;$set.$.fitness&#39;</span><span class="p">,</span> <span class="n">fitness</span><span class="p">)</span>

            <span class="n">weighted_fitness</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">fitness</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizee_fitness_weights</span><span class="p">)</span>
            <span class="n">weighted_fitness_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">weighted_fitness</span><span class="p">)</span>

            <span class="c1"># The last element of the list is the evaluation of the individual obtained via gradient descent</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">fitnesses_results</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">current_fitness</span> <span class="o">=</span> <span class="n">weighted_fitness</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">fitnesses</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">weighted_fitness</span>
                <span class="n">dx</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dict_to_list</span><span class="p">(</span><span class="n">individual</span><span class="p">))</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">current_individual</span>
        <span class="n">traj</span><span class="o">.</span><span class="n">v_idx</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>  <span class="c1"># set the trajectory back to default</span>

        <span class="c1"># Performs descending arg-sort of weighted fitness</span>
        <span class="n">fitness_sorting_indices</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">weighted_fitness_list</span><span class="p">)))</span>
        <span class="n">old_eval_pop_as_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">dict_to_list</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">old_eval_pop</span><span class="p">])</span>

        <span class="c1"># Sorting the data according to fitness</span>
        <span class="n">sorted_population</span> <span class="o">=</span> <span class="n">old_eval_pop_as_array</span><span class="p">[</span><span class="n">fitness_sorting_indices</span><span class="p">]</span>
        <span class="n">sorted_fitness</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">weighted_fitness_list</span><span class="p">)[</span><span class="n">fitness_sorting_indices</span><span class="p">]</span>

        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;-- End of generation </span><span class="si">%d</span><span class="s2"> --&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">g</span><span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;  Evaluated </span><span class="si">%d</span><span class="s2"> individuals&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">fitnesses_results</span><span class="p">))</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;  Average Fitness: </span><span class="si">%.4f</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">sorted_fitness</span><span class="p">))</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;  Current fitness is </span><span class="si">%.2f</span><span class="s2">&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">current_fitness</span><span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;  Best Fitness: </span><span class="si">%.4f</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">sorted_fitness</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;  Best individual is </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">sorted_population</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

        <span class="n">generation_result_dict</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;generation&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">g</span><span class="p">,</span>
            <span class="s1">&#39;current_fitness&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">current_fitness</span><span class="p">,</span>
            <span class="s1">&#39;best_fitness_in_run&#39;</span><span class="p">:</span> <span class="n">sorted_fitness</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
            <span class="s1">&#39;average_fitness_in_run&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">sorted_fitness</span><span class="p">),</span>
        <span class="p">}</span>

        <span class="n">generation_name</span> <span class="o">=</span> <span class="s1">&#39;generation_</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">g</span><span class="p">)</span>
        <span class="n">traj</span><span class="o">.</span><span class="n">results</span><span class="o">.</span><span class="n">generation_params</span><span class="o">.</span><span class="n">f_add_result_group</span><span class="p">(</span><span class="n">generation_name</span><span class="p">)</span>
        <span class="n">traj</span><span class="o">.</span><span class="n">results</span><span class="o">.</span><span class="n">generation_params</span><span class="o">.</span><span class="n">f_add_result</span><span class="p">(</span>
            <span class="n">generation_name</span> <span class="o">+</span> <span class="s1">&#39;.algorithm_params&#39;</span><span class="p">,</span> <span class="n">generation_result_dict</span><span class="p">)</span>

        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;-- End of iteration </span><span class="si">{}</span><span class="s2">, current fitness is </span><span class="si">{}</span><span class="s2"> --&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">g</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">current_fitness</span><span class="p">))</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">g</span> <span class="o">&lt;</span> <span class="n">traj</span><span class="o">.</span><span class="n">n_iteration</span> <span class="o">-</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">traj</span><span class="o">.</span><span class="n">stop_criterion</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">current_fitness</span><span class="p">:</span>
            <span class="c1"># Create new individual using the appropriate gradient descent</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">update_function</span><span class="p">(</span><span class="n">traj</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">pinv</span><span class="p">(</span><span class="n">dx</span><span class="p">),</span> <span class="n">fitnesses</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">current_fitness</span><span class="p">))</span>
            <span class="n">current_individual_dict</span> <span class="o">=</span> <span class="n">list_to_dict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">current_individual</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizee_individual_dict_spec</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizee_bounding_func</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">current_individual_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizee_bounding_func</span><span class="p">(</span><span class="n">current_individual_dict</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">current_individual</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dict_to_list</span><span class="p">(</span><span class="n">current_individual_dict</span><span class="p">))</span>

            <span class="c1"># Explore the neighbourhood in the parameter space of the current individual</span>
            <span class="n">new_individual_list</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">list_to_dict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">current_individual</span> <span class="o">+</span> 
                             <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">traj</span><span class="o">.</span><span class="n">exploration_step_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">current_individual</span><span class="o">.</span><span class="n">size</span><span class="p">),</span>
                             <span class="bp">self</span><span class="o">.</span><span class="n">optimizee_individual_dict_spec</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">traj</span><span class="o">.</span><span class="n">n_random_steps</span><span class="p">)</span>
            <span class="p">]</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizee_bounding_func</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">new_individual_list</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizee_bounding_func</span><span class="p">(</span><span class="n">ind</span><span class="p">)</span> <span class="k">for</span> <span class="n">ind</span> <span class="ow">in</span> <span class="n">new_individual_list</span><span class="p">]</span>
            <span class="n">new_individual_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_individual_dict</span><span class="p">)</span>

            <span class="n">fitnesses_results</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">eval_pop</span> <span class="o">=</span> <span class="n">new_individual_list</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">g</span> <span class="o">+=</span> <span class="mi">1</span>  <span class="c1"># Update generation counter</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_expand_trajectory</span><span class="p">(</span><span class="n">traj</span><span class="p">)</span></div>

<div class="viewcode-block" id="GradientDescentOptimizer.end"><a class="viewcode-back" href="../../../../ltl.optimizers.gradientdescent.html#ltl.optimizers.gradientdescent.optimizer.GradientDescentOptimizer.end">[docs]</a>    <span class="k">def</span> <span class="nf">end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">traj</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        See :meth:`~ltl.optimizers.optimizer.Optimizer.end`</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">best_last_indiv_dict</span> <span class="o">=</span> <span class="n">list_to_dict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">current_individual</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span>
                                            <span class="bp">self</span><span class="o">.</span><span class="n">optimizee_individual_dict_spec</span><span class="p">)</span>

        <span class="n">traj</span><span class="o">.</span><span class="n">f_add_result</span><span class="p">(</span><span class="s1">&#39;final_individual&#39;</span><span class="p">,</span> <span class="n">best_last_indiv_dict</span><span class="p">)</span>
        <span class="n">traj</span><span class="o">.</span><span class="n">f_add_result</span><span class="p">(</span><span class="s1">&#39;final_fitness&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">current_fitness</span><span class="p">)</span>
        <span class="n">traj</span><span class="o">.</span><span class="n">f_add_result</span><span class="p">(</span><span class="s1">&#39;n_iteration&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">g</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;The last individual was </span><span class="si">%s</span><span class="s2"> with fitness </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">current_individual</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">current_fitness</span><span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;-- End of (successful) gradient descent --&quot;</span><span class="p">)</span></div>

<div class="viewcode-block" id="GradientDescentOptimizer.init_classic_gd"><a class="viewcode-back" href="../../../../ltl.optimizers.gradientdescent.html#ltl.optimizers.gradientdescent.optimizer.GradientDescentOptimizer.init_classic_gd">[docs]</a>    <span class="k">def</span> <span class="nf">init_classic_gd</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">traj</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Classic Gradient Descent specific initializiation.</span>

<span class="sd">        :param ~pypet.trajectory.Trajectory traj: The :mod:&#39;pypet&#39; trajectory on which the parameters should get stored.</span>

<span class="sd">        :return:</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">update_function</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classic_gd_update</span></div>
    
<div class="viewcode-block" id="GradientDescentOptimizer.init_rmsprop"><a class="viewcode-back" href="../../../../ltl.optimizers.gradientdescent.html#ltl.optimizers.gradientdescent.optimizer.GradientDescentOptimizer.init_rmsprop">[docs]</a>    <span class="k">def</span> <span class="nf">init_rmsprop</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">traj</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        RMSProp specific initializiation.</span>

<span class="sd">        :param ~pypet.trajectory.Trajectory traj: The :mod:&#39;pypet&#39; trajectory on which the parameters should get stored.</span>

<span class="sd">        :return:</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">update_function</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rmsprop_update</span>

        <span class="n">traj</span><span class="o">.</span><span class="n">f_add_parameter</span><span class="p">(</span><span class="s1">&#39;momentum_decay&#39;</span><span class="p">,</span> <span class="n">parameters</span><span class="o">.</span><span class="n">momentum_decay</span><span class="p">,</span> 
                             <span class="n">comment</span><span class="o">=</span><span class="s1">&#39;Decay of the historic momentum at each gradient descent step&#39;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">delta</span> <span class="o">=</span> <span class="mi">10</span><span class="o">**</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">)</span>  <span class="c1"># used to for numerical stabilization</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">so_moment</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">current_individual</span><span class="p">))</span>  <span class="c1"># second order moment</span></div>

<div class="viewcode-block" id="GradientDescentOptimizer.init_adam"><a class="viewcode-back" href="../../../../ltl.optimizers.gradientdescent.html#ltl.optimizers.gradientdescent.optimizer.GradientDescentOptimizer.init_adam">[docs]</a>    <span class="k">def</span> <span class="nf">init_adam</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">traj</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        ADAM specific initializiation.</span>

<span class="sd">        :param ~pypet.trajectory.Trajectory traj: The :mod:&#39;pypet&#39; trajectory on which the parameters should get stored.</span>

<span class="sd">        :return:</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">update_function</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">adam_update</span>

        <span class="n">traj</span><span class="o">.</span><span class="n">f_add_parameter</span><span class="p">(</span><span class="s1">&#39;first_order_decay&#39;</span><span class="p">,</span> <span class="n">parameters</span><span class="o">.</span><span class="n">first_order_decay</span><span class="p">,</span> 
                             <span class="n">comment</span><span class="o">=</span><span class="s1">&#39;Decay of the first order momentum&#39;</span><span class="p">)</span>
        <span class="n">traj</span><span class="o">.</span><span class="n">f_add_parameter</span><span class="p">(</span><span class="s1">&#39;second_order_decay&#39;</span><span class="p">,</span> <span class="n">parameters</span><span class="o">.</span><span class="n">second_order_decay</span><span class="p">,</span> 
                             <span class="n">comment</span><span class="o">=</span><span class="s1">&#39;Decay of the second order momentum&#39;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">delta</span> <span class="o">=</span> <span class="mi">10</span><span class="o">**</span><span class="p">(</span><span class="o">-</span><span class="mi">8</span><span class="p">)</span>  <span class="c1"># used for numerical stablization</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fo_moment</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">current_individual</span><span class="p">))</span>  <span class="c1"># first order moment</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">so_moment</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">current_individual</span><span class="p">))</span>  <span class="c1"># second order moment</span></div>

<div class="viewcode-block" id="GradientDescentOptimizer.init_stochastic_gd"><a class="viewcode-back" href="../../../../ltl.optimizers.gradientdescent.html#ltl.optimizers.gradientdescent.optimizer.GradientDescentOptimizer.init_stochastic_gd">[docs]</a>    <span class="k">def</span> <span class="nf">init_stochastic_gd</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">traj</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Stochastic Gradient Descent specific initializiation.</span>

<span class="sd">        :param ~pypet.trajectory.Trajectory traj: The :mod:&#39;pypet&#39; trajectory on which the parameters should get stored.</span>

<span class="sd">        :return:</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">update_function</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stochastic_gd_update</span>

        <span class="n">traj</span><span class="o">.</span><span class="n">f_add_parameter</span><span class="p">(</span><span class="s1">&#39;stochastic_deviation&#39;</span><span class="p">,</span> <span class="n">parameters</span><span class="o">.</span><span class="n">stochastic_deviation</span><span class="p">,</span> 
                             <span class="n">comment</span><span class="o">=</span><span class="s1">&#39;Standard deviation of the random vector added to the gradient&#39;</span><span class="p">)</span>
        <span class="n">traj</span><span class="o">.</span><span class="n">f_add_parameter</span><span class="p">(</span><span class="s1">&#39;stochastic_decay&#39;</span><span class="p">,</span> <span class="n">parameters</span><span class="o">.</span><span class="n">stochastic_decay</span><span class="p">,</span> <span class="n">comment</span><span class="o">=</span><span class="s1">&#39;Decay of the random vector&#39;</span><span class="p">)</span></div>

<div class="viewcode-block" id="GradientDescentOptimizer.classic_gd_update"><a class="viewcode-back" href="../../../../ltl.optimizers.gradientdescent.html#ltl.optimizers.gradientdescent.optimizer.GradientDescentOptimizer.classic_gd_update">[docs]</a>    <span class="k">def</span> <span class="nf">classic_gd_update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">traj</span><span class="p">,</span> <span class="n">gradient</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Updates the current individual using the classic Gradient Descent algorithm.</span>

<span class="sd">        :param ~pypet.trajectory.Trajectory traj: The :mod:&#39;pypet&#39; trajectory which contains the parameters </span>
<span class="sd">            required by the update algorithm</span>

<span class="sd">        :param ~numpy.ndarray gradient: The gradient of the fitness curve, evaluated at the current individual</span>

<span class="sd">        :return:</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">current_individual</span> <span class="o">+=</span> <span class="n">traj</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">gradient</span></div>

<div class="viewcode-block" id="GradientDescentOptimizer.rmsprop_update"><a class="viewcode-back" href="../../../../ltl.optimizers.gradientdescent.html#ltl.optimizers.gradientdescent.optimizer.GradientDescentOptimizer.rmsprop_update">[docs]</a>    <span class="k">def</span> <span class="nf">rmsprop_update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">traj</span><span class="p">,</span> <span class="n">gradient</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Updates the current individual using the RMSProp algorithm.</span>

<span class="sd">        :param ~pypet.trajectory.Trajectory traj: The :mod:&#39;pypet&#39; trajectory which contains the parameters </span>
<span class="sd">            required by the update algorithm</span>

<span class="sd">        :param ~numpy.ndarray gradient: The gradient of the fitness curve, evaluated at the current individual</span>

<span class="sd">        :return:</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">so_moment</span> <span class="o">=</span> <span class="p">(</span><span class="n">traj</span><span class="o">.</span><span class="n">momentum_decay</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">so_moment</span> <span class="o">+</span> 
                          <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">traj</span><span class="o">.</span><span class="n">momentum_decay</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">gradient</span><span class="p">,</span> <span class="n">gradient</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">current_individual</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">traj</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">so_moment</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">delta</span><span class="p">)),</span>
                                               <span class="n">gradient</span><span class="p">)</span></div>

<div class="viewcode-block" id="GradientDescentOptimizer.adam_update"><a class="viewcode-back" href="../../../../ltl.optimizers.gradientdescent.html#ltl.optimizers.gradientdescent.optimizer.GradientDescentOptimizer.adam_update">[docs]</a>    <span class="k">def</span> <span class="nf">adam_update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">traj</span><span class="p">,</span> <span class="n">gradient</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Updates the current individual using the ADAM algorithm.</span>

<span class="sd">        :param ~pypet.trajectory.Trajectory traj: The :mod:&#39;pypet&#39; trajectory which contains the parameters </span>
<span class="sd">            required by the update algorithm</span>

<span class="sd">        :param ~numpy.ndarray gradient: The gradient of the fitness curve, evaluated at the current individual</span>

<span class="sd">        :return:</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">fo_moment</span> <span class="o">=</span> <span class="p">(</span><span class="n">traj</span><span class="o">.</span><span class="n">first_order_decay</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">fo_moment</span> <span class="o">+</span>
                          <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">traj</span><span class="o">.</span><span class="n">first_order_decay</span><span class="p">)</span> <span class="o">*</span> <span class="n">gradient</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">so_moment</span> <span class="o">=</span> <span class="p">(</span><span class="n">traj</span><span class="o">.</span><span class="n">second_order_decay</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">so_moment</span> <span class="o">+</span>
                          <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">traj</span><span class="o">.</span><span class="n">second_order_decay</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">gradient</span><span class="p">,</span> <span class="n">gradient</span><span class="p">))</span>
        <span class="n">fo_moment_corrected</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fo_moment</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">traj</span><span class="o">.</span><span class="n">first_order_decay</span> <span class="o">**</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">g</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">so_moment_corrected</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">so_moment</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">traj</span><span class="o">.</span><span class="n">second_order_decay</span> <span class="o">**</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">g</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">current_individual</span> <span class="o">+=</span> <span class="n">traj</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">fo_moment_corrected</span> <span class="o">/</span> \
                                    <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">so_moment_corrected</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">delta</span><span class="p">)</span></div>

<div class="viewcode-block" id="GradientDescentOptimizer.stochastic_gd_update"><a class="viewcode-back" href="../../../../ltl.optimizers.gradientdescent.html#ltl.optimizers.gradientdescent.optimizer.GradientDescentOptimizer.stochastic_gd_update">[docs]</a>    <span class="k">def</span> <span class="nf">stochastic_gd_update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">traj</span><span class="p">,</span> <span class="n">gradient</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Updates the current individual using a stochastic version of the gradient descent algorithm.</span>

<span class="sd">        :param ~pypet.trajectory.Trajectory traj: The :mod:&#39;pypet&#39; trajectory which contains the parameters </span>
<span class="sd">            required by the update algorithm</span>

<span class="sd">        :param ~numpy.ndarray gradient: The gradient of the fitness curve, evaluated at the current individual</span>

<span class="sd">        :return:</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">gradient</span> <span class="o">+=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">traj</span><span class="o">.</span><span class="n">stochastic_deviation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">current_individual</span><span class="o">.</span><span class="n">size</span><span class="p">)</span> <span class="o">*</span> 
                     <span class="n">traj</span><span class="o">.</span><span class="n">stochastic_decay</span><span class="o">**</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">g</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">current_individual</span> <span class="o">+=</span> <span class="n">traj</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">gradient</span></div></div>
</pre></div>

                </div>
              </div>
            </div>
          </div>
        </div>
        
        
      </div><!-- /row -->

      <!-- row -->
      <div class="row footer-relbar">
<div id="navbar-related" class=" related navbar navbar-default" role="navigation" aria-label="related navigation">
  <div class="navbar-inner">
    <ul class="nav navbar-nav ">
        <li><a href="../../../../index.html">LTL 1.0.0-beta documentation</a></li>
    </ul>
<ul class="nav navbar-nav pull-right hidden-xs hidden-sm">
      
        <li><a href="../../../../py-modindex.html" title="Python Module Index" >modules</a></li>
        <li><a href="../../../../genindex.html" title="General Index" >index</a></li>
        <li><a href="../../../index.html" >Module code</a></li>
        <li><a href="../../../ltl.html" >ltl</a></li>
        <li><a href="#">top</a></li> 
      
    </ul>
  </div>
</div>
      </div><!-- /row -->

      <!-- footer -->
      <footer role="contentinfo">
          &copy; Copyright 2017, Anand Subramoney.
        Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 1.8.1.
      </footer>
      <!-- /footer -->

    </div>
    <!-- /container -->

  </body>
</html>